{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSC-180 Project 2\n",
    "##### By: Derek Chen, Nicolas Gueliemo, Xai Yang, Katrina Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Provided Functions from Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "from matplotlib.pyplot import figure, show\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column. \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if isinstance(target_type, Sequence) else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        print('CLASS')\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        print('REG')\n",
    "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "# Function to plot results\n",
    "def plot(results):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    for activation in results['Activation'].unique():\n",
    "        subset = results[results['Activation'] == activation]\n",
    "        ax.plot(subset['Neurons'].astype(str), subset['RMSE'], marker='o', label=activation)\n",
    "\n",
    "    ax.set_title('Model Performance by Activation Function')\n",
    "    ax.set_xlabel('Layer Configuration (Neurons)')\n",
    "    ax.set_ylabel('RMSE')\n",
    "    ax.legend(title='Activation Function')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    plt.plot(t['pred'].tolist(), label='Prediction', color='orange')\n",
    "    plt.plot(t['y'].tolist(), label='Expected', color='blue')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Processing\n",
    "#### By: Nicolas Gugliemo\n",
    "#### Includes:\n",
    "- Read from CSV Files\n",
    "- Create Data frames\n",
    "- Clean Data to drop dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_cat\n",
      "Normal            58984\n",
      "Exploits          16187\n",
      "DoS                1791\n",
      "Fuzzers            1731\n",
      "Reconnaissance     1703\n",
      "Analysis            564\n",
      "Worms               114\n",
      "Backdoor             99\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\AppData\\Local\\Temp\\ipykernel_19652\\2628312653.py:33: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df['attack_cat'].replace('Generic', 'Normal', inplace=True)\n",
      "C:\\Users\\nicol\\AppData\\Local\\Temp\\ipykernel_19652\\2628312653.py:34: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df['attack_cat'].replace('Generic', 'Normal', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Set paths and set option to print entire tables\n",
    "path = \"./data/\"\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "#Set paths for CSV\n",
    "features_path = os.path.join(path,\"NUSW-NB15_features.csv\")\n",
    "train_path = os.path.join(path,\"UNSW_NB15_training-set.csv\")\n",
    "test_path = os.path.join(path,\"UNSW_NB15_test-set.csv\")\n",
    "\n",
    "#Read the CSVs \n",
    "features_df = pd.read_csv(features_path,encoding='ISO-8859-1')\n",
    "train_df = pd.read_csv(train_path,encoding='ISO-8859-1')\n",
    "test_df = pd.read_csv(test_path,encoding='ISO-8859-1')\n",
    "\n",
    "#Fix ID's Name\n",
    "train_df.rename(columns={'ï»¿id': 'id'}, inplace=True)\n",
    "test_df.rename(columns={'ï»¿id': 'id'}, inplace=True)\n",
    "\n",
    "#Replace '-' (missing value) with NA\n",
    "train_df.replace('-', pd.NA, inplace=True)\n",
    "test_df.replace('-', pd.NA, inplace=True)\n",
    "  \n",
    "#Drop missing values\n",
    "features_df = features_df.dropna()\n",
    "train_df = train_df.dropna()\n",
    "test_df = test_df.dropna()\n",
    "\n",
    "train_df['attack_cat'].replace('Generic', 'Normal', inplace=True)\n",
    "test_df['attack_cat'].replace('Generic', 'Normal', inplace=True)\n",
    "##TO-DO, DROP UNIQUE ROWS\n",
    "train_df[0:10]\n",
    "\n",
    "print(train_df['attack_cat'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical features and normalize numeric features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_cat\n",
      "5    58984\n",
      "3    16187\n",
      "2     1791\n",
      "4     1731\n",
      "6     1703\n",
      "0      564\n",
      "7      114\n",
      "1       99\n",
      "Name: count, dtype: int64\n",
      "attack_cat\n",
      "4    28085\n",
      "2     5293\n",
      "1      717\n",
      "3      535\n",
      "5      504\n",
      "6       34\n",
      "0       11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Test the expected categorical columns to one hot encoding \n",
    "pd.get_dummies(train_df, columns=['proto', 'service', 'state']).head()\n",
    "pd.get_dummies(test_df, columns=['proto', 'service', 'state']).head()\n",
    "\n",
    "# Send the expected numeric and categorical columns to normalization. Send target to text_index.  \n",
    "NumericColumns = ['dur','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','tcprtt',\n",
    "               'synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd',\n",
    "               'ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports']\n",
    "CategoricalColumns = ['proto', 'service', 'state']\n",
    "for i in NumericColumns:\n",
    "    encode_numeric_zscore(train_df,i)\n",
    "    encode_numeric_zscore(test_df,i)\n",
    "for i in CategoricalColumns:\n",
    "    encode_text_dummy(train_df,i)\n",
    "    encode_text_dummy(test_df,i)\n",
    "encode_text_index(train_df,'attack_cat')\n",
    "encode_text_index(test_df,'attack_cat') #\n",
    "print(train_df['attack_cat'].value_counts())\n",
    "print(test_df['attack_cat'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Unique Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories in Train: {7}\n",
      "Unique categories in Test: set()\n",
      "Train categories: 7\n",
      "Test categories: 7\n",
      "attack_cat\n",
      "5    58984\n",
      "3    16187\n",
      "2     1791\n",
      "4     1731\n",
      "6     1703\n",
      "0      564\n",
      "1       99\n",
      "Name: count, dtype: int64\n",
      "attack_cat\n",
      "4    28085\n",
      "2     5293\n",
      "1      717\n",
      "3      535\n",
      "5      504\n",
      "6       34\n",
      "0       11\n",
      "Name: count, dtype: int64\n",
      "Before fix [5 1 4 6 3 0 2]\n",
      "attack_cat\n",
      "1    58984\n",
      "0    22075\n",
      "Name: count, dtype: int64\n",
      "attack_cat\n",
      "1    28085\n",
      "0     7094\n",
      "Name: count, dtype: int64\n",
      "[1 0]\n",
      "Updated Train categories: 2\n",
      "Updated Test categories: 2\n"
     ]
    }
   ],
   "source": [
    "train_categories = set(train_df['attack_cat'].unique())\n",
    "test_categories = set(test_df['attack_cat'].unique())\n",
    "\n",
    "unique_to_train = train_categories - test_categories\n",
    "unique_to_test = test_categories - train_categories\n",
    "\n",
    "print(\"Unique categories in Train:\", unique_to_train)\n",
    "print(\"Unique categories in Test:\", unique_to_test)\n",
    "\n",
    "# Drop unique categories in the train dataset\n",
    "train_df = train_df[~train_df['attack_cat'].isin(unique_to_train)]\n",
    "\n",
    "# Drop unique categories in the test dataset\n",
    "test_df = test_df[~test_df['attack_cat'].isin(unique_to_test)]\n",
    "\n",
    "print(\"Train categories:\", train_df['attack_cat'].nunique())\n",
    "print(\"Test categories:\", test_df['attack_cat'].nunique())\n",
    "print(train_df['attack_cat'].value_counts())\n",
    "print(test_df['attack_cat'].value_counts())\n",
    "print(\"Before fix\",train_df['attack_cat'].unique())\n",
    "\n",
    "#Fix to binary problem where 1 means safe and 0 means attack\n",
    "train_df['attack_cat'] = train_df['attack_cat'].apply(lambda x: 1 if x == 5 else 0)\n",
    "test_df['attack_cat'] = test_df['attack_cat'].apply(lambda x: 1 if x == 4 else 0)\n",
    "print(train_df['attack_cat'].value_counts())\n",
    "print(test_df['attack_cat'].value_counts())\n",
    "print(train_df['attack_cat'].unique())\n",
    "\n",
    "print(\"Updated Train categories:\", train_df['attack_cat'].nunique())\n",
    "print(\"Updated Test categories:\", test_df['attack_cat'].nunique())\n",
    "# Assuming you have a DataFrame called df and want to print unique values in a specific column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS\n",
      "CLASS\n",
      "Unique values in y_train_binary: [0 1]\n",
      "Counts in y_train_binary: [22075 58984]\n",
      "\n",
      "Unique values in y_test_binary: [0 1]\n",
      "Counts in y_test_binary: [ 7094 28085]\n",
      "Train X shape: (81059, 60)\n",
      "Train Y shape: (81059,)\n",
      "Test X shape: (35179, 60)\n",
      "Test Y shape: (35179,)\n",
      "attack_cat\n",
      "1    58984\n",
      "0    22075\n",
      "Name: count, dtype: int64\n",
      "attack_cat\n",
      "1    28085\n",
      "0     7094\n",
      "Name: count, dtype: int64\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicol\\OneDrive\\Desktop\\CodingFiles\\CSC 180\\.conda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2534/2534 - 4s - 2ms/step - loss: 0.6046 - val_loss: 0.5179\n",
      "Epoch 2/1000\n",
      "2534/2534 - 3s - 1ms/step - loss: 0.5856 - val_loss: 0.5159\n",
      "Epoch 3/1000\n",
      "2534/2534 - 3s - 1ms/step - loss: 0.5856 - val_loss: 0.5167\n",
      "Epoch 4/1000\n",
      "2534/2534 - 3s - 1ms/step - loss: 0.5856 - val_loss: 0.5167\n",
      "Epoch 5/1000\n",
      "2534/2534 - 3s - 1ms/step - loss: 0.5856 - val_loss: 0.5168\n",
      "Epoch 6/1000\n",
      "2534/2534 - 4s - 1ms/step - loss: 0.5856 - val_loss: 0.5162\n",
      "Epoch 7/1000\n",
      "2534/2534 - 3s - 1ms/step - loss: 0.5856 - val_loss: 0.5170\n",
      "Epoch 7: early stopping\n",
      "\u001b[1m  74/1100\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 685us/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicol\\OneDrive\\Desktop\\CodingFiles\\CSC 180\\.conda\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 641us/step\n",
      "Accuracy score: 0.20165439608857558\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      1.00      0.34      7094\n",
      "           1       0.00      0.00      0.00     28085\n",
      "\n",
      "    accuracy                           0.20     35179\n",
      "   macro avg       0.10      0.50      0.17     35179\n",
      "weighted avg       0.04      0.20      0.07     35179\n",
      "\n",
      "\u001b[1m 150/1100\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 682us/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicol\\OneDrive\\Desktop\\CodingFiles\\CSC 180\\.conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nicol\\OneDrive\\Desktop\\CodingFiles\\CSC 180\\.conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nicol\\OneDrive\\Desktop\\CodingFiles\\CSC 180\\.conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 571us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.20      1.00      0.34      7094\n",
      "      Attack       0.00      0.00      0.00     28085\n",
      "\n",
      "    accuracy                           0.20     35179\n",
      "   macro avg       0.10      0.50      0.17     35179\n",
      "weighted avg       0.04      0.20      0.07     35179\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicol\\OneDrive\\Desktop\\CodingFiles\\CSC 180\\.conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nicol\\OneDrive\\Desktop\\CodingFiles\\CSC 180\\.conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nicol\\OneDrive\\Desktop\\CodingFiles\\CSC 180\\.conda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#Figure out how to turn the output into 0 and 1, not 0-7\n",
    "x,y = to_xy(train_df,\"attack_cat\")\n",
    "x_test,y_test = to_xy(test_df,\"attack_cat\")\n",
    "\n",
    "y = np.argmax(y, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "unique_train, counts_train = np.unique(y, return_counts=True)\n",
    "print(\"Unique values in y_train_binary:\", unique_train)\n",
    "print(\"Counts in y_train_binary:\", counts_train)\n",
    "\n",
    "# Print unique values and their counts for y_test_binary\n",
    "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "print(\"\\nUnique values in y_test_binary:\", unique_test)\n",
    "print(\"Counts in y_test_binary:\", counts_test)\n",
    "print(\"Train X shape:\", x.shape)\n",
    "print(\"Train Y shape:\", y.shape)\n",
    "print(\"Test X shape:\", x_test.shape)\n",
    "print(\"Test Y shape:\", y_test.shape)\n",
    "print(train_df['attack_cat'].value_counts())\n",
    "print(test_df['attack_cat'].value_counts())\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=x.shape[1], activation='relu')) # More neurons\n",
    "model.add(Dropout(0.1))  # Dropout to reduce overfitting\n",
    "model.add(Dense(50, activation='relu')) # Additional layer\n",
    "model.add(Dropout(0.1))  # Dropout to reduce overfitting\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(0.1))  # Dropout to reduce overfitting\n",
    "model.add(Dense(1, activation='sigmoid')) # Output layer\n",
    "\n",
    "adam = optimizers.Adam(learning_rate=0.001, beta_1=0.999, beta_2=0.999, epsilon=None, amsgrad=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=2, mode='auto')  \n",
    "model.save('dnn/model.keras')\n",
    "checkpointer = ModelCheckpoint(filepath=\"dnn/model.keras\", verbose=0, save_best_only=True) # save best model\n",
    "model.fit(x, y, validation_data=(x_test,y_test), callbacks=[monitor], verbose=2, epochs=1000) \n",
    "model.load_weights('dnn/model.keras') # load weights from best model \n",
    "#model.fit(x,y,verbose=2,epochs=10)\n",
    "\n",
    "# Make predictions on the test set\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "predicted_classes = (pred > 0.5).astype(int).flatten()  # Ensure it's 1D\n",
    "true_classes = y_test.flatten()  # Ensure it's 1D\n",
    "\n",
    "# Evaluate the accuracy score\n",
    "score = metrics.accuracy_score(true_classes, predicted_classes)\n",
    "print(\"Accuracy score: {}\".format(score))\n",
    "\n",
    "# Print classification report\n",
    "print(metrics.classification_report(true_classes, predicted_classes))\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = model.predict(x_test).flatten()\n",
    "y_pred_classes = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_classes, target_names=['Normal', 'Attack']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
