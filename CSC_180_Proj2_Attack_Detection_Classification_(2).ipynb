{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d45iZ5bY4Kyn"
      },
      "source": [
        "## CSC-180 Project 2\n",
        "##### By: Derek Chen, Nicolas Gueliemo, Xai Yang, Katrina Yu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG34G4-Q4Kyp"
      },
      "source": [
        "### Imports & Provided Functions from Lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "roXlLKf44Kyp"
      },
      "outputs": [],
      "source": [
        "from collections.abc import Sequence\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "from matplotlib.pyplot import figure, show\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "\n",
        "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
        "def encode_text_dummy(df, name):\n",
        "    dummies = pd.get_dummies(df[name])\n",
        "    for x in dummies.columns:\n",
        "        dummy_name = \"{}-{}\".format(name, x)\n",
        "        df[dummy_name] = dummies[x]\n",
        "    df.drop(name, axis=1, inplace=True)\n",
        "\n",
        "\n",
        "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
        "def encode_text_index(df, name):\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    df[name] = le.fit_transform(df[name])\n",
        "    return le.classes_\n",
        "\n",
        "\n",
        "# Encode a numeric column as zscores\n",
        "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
        "    if mean is None:\n",
        "        mean = df[name].mean()\n",
        "\n",
        "    if sd is None:\n",
        "        sd = df[name].std()\n",
        "\n",
        "    df[name] = (df[name] - mean) / sd\n",
        "\n",
        "\n",
        "# Convert all missing values in the specified column to the median\n",
        "def missing_median(df, name):\n",
        "    med = df[name].median()\n",
        "    df[name] = df[name].fillna(med)\n",
        "\n",
        "\n",
        "# Convert all missing values in the specified column to the default\n",
        "def missing_default(df, name, default_value):\n",
        "    df[name] = df[name].fillna(default_value)\n",
        "\n",
        "\n",
        "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
        "def to_xy(df, target):\n",
        "    result = []\n",
        "    for x in df.columns:\n",
        "        if x != target:\n",
        "            result.append(x)\n",
        "    # find out the type of the target column.\n",
        "    target_type = df[target].dtypes\n",
        "    target_type = target_type[0] if isinstance(target_type, Sequence) else target_type\n",
        "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
        "    if target_type in (np.int64, np.int32):\n",
        "        # Classification\n",
        "        dummies = pd.get_dummies(df[target])\n",
        "        print('CLASS')\n",
        "        unique_values = df[target].nunique()\n",
        "        if unique_values > 2:\n",
        "            # If more than 2 classes, convert to binary (e.g., 0 for normal, 1 for attacks)\n",
        "            df[target] = np.where(df[target] > 0, 1, 0)\n",
        "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32).reshape(-1, 1)\n",
        "    else:\n",
        "        # Regression\n",
        "        print('REG')\n",
        "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)\n",
        "\n",
        "# Nicely formatted time string\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
        "\n",
        "# Function to plot results\n",
        "def plot(results):\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    for activation in results['Activation'].unique():\n",
        "        subset = results[results['Activation'] == activation]\n",
        "        ax.plot(subset['Neurons'].astype(str), subset['RMSE'], marker='o', label=activation)\n",
        "\n",
        "    ax.set_title('Model Performance by Activation Function')\n",
        "    ax.set_xlabel('Layer Configuration (Neurons)')\n",
        "    ax.set_ylabel('RMSE')\n",
        "    ax.legend(title='Activation Function')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def chart_regression(pred,y,sort=True):\n",
        "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
        "    if sort:\n",
        "        t.sort_values(by=['y'],inplace=True)\n",
        "    plt.plot(t['pred'].tolist(), label='Prediction', color='orange')\n",
        "    plt.plot(t['y'].tolist(), label='Expected', color='blue')\n",
        "    plt.ylabel('output')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Remove all rows where the specified column is +/- sd standard deviations\n",
        "def remove_outliers(df, name, sd):\n",
        "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
        "    df.drop(drop_rows, axis=0, inplace=True)\n",
        "\n",
        "\n",
        "# Encode a column to a range between normalized_low and normalized_high.\n",
        "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
        "                         data_low=None, data_high=None):\n",
        "    if data_low is None:\n",
        "        data_low = min(df[name])\n",
        "        data_high = max(df[name])\n",
        "\n",
        "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
        "               * (normalized_high - normalized_low) + normalized_low\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb6eCHuA4Kyr"
      },
      "source": [
        "### Step 1: Data Processing\n",
        "#### By: Nicolas Gugliemo\n",
        "#### Includes:\n",
        "- Read from CSV Files\n",
        "- Create Data frames\n",
        "- Clean Data to drop dups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhmBwlJo4Kyr",
        "outputId": "6be6b15c-e7f2-41f1-a6d6-f6844f8d939d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Values:\n",
            "attack_cat\n",
            "1    61685\n",
            "0    19488\n",
            "Name: count, dtype: int64\n",
            "Test Values:\n",
            "attack_cat\n",
            "1    25554\n",
            "0     9625\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#Set paths and set option to print entire tables\n",
        "path = \"./data/\"\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "#Set paths for CSV\n",
        "features_path = os.path.join(path,\"NUSW-NB15_features.csv\")\n",
        "train_path = os.path.join(path,\"UNSW_NB15_training-set.csv\")\n",
        "test_path = os.path.join(path,\"UNSW_NB15_test-set.csv\")\n",
        "\n",
        "#Read the CSVs\n",
        "features_df = pd.read_csv(features_path,encoding='ISO-8859-1')\n",
        "train_df = pd.read_csv(train_path,encoding='ISO-8859-1')\n",
        "test_df = pd.read_csv(test_path,encoding='ISO-8859-1')\n",
        "\n",
        "#Fix ID's Name\n",
        "train_df.rename(columns={'ï»¿id': 'id'}, inplace=True)\n",
        "test_df.rename(columns={'ï»¿id': 'id'}, inplace=True)\n",
        "\n",
        "#Replace '-' (missing value) with NA\n",
        "train_df.replace('-', pd.NA, inplace=True)\n",
        "test_df.replace('-', pd.NA, inplace=True)\n",
        "\n",
        "#Drop missing values\n",
        "features_df = features_df.dropna()\n",
        "train_df = train_df.dropna()\n",
        "test_df = test_df.dropna()\n",
        "\n",
        "train_df['attack_cat'] = train_df['attack_cat'].apply(lambda x: 1 if x != 'Normal' else 0)\n",
        "test_df['attack_cat'] = test_df['attack_cat'].apply(lambda x: 1 if x != 'Normal' else 0)\n",
        "\n",
        "print('Train Values:')\n",
        "print(train_df['attack_cat'].value_counts())\n",
        "print('Test Values:')\n",
        "print(test_df['attack_cat'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWwBCABi4Kys"
      },
      "source": [
        "### Encode categorical features and normalize numeric features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "QN0xz15j4Kys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "ff0bbc00-5704-49bd-902b-f340a8b509d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLASS\n",
            "CLASS\n",
            "attack_cat\n",
            "1    61681\n",
            "0    19478\n",
            "Name: count, dtype: int64\n",
            "attack_cat\n",
            "1    25554\n",
            "0     9624\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#DROP UNIQUE ROWS\n",
        "CategoricalColumns = ['proto', 'service', 'state']\n",
        "\n",
        "# One-hot encoding for the categorical columns using encode_text_dummy\n",
        "for col in CategoricalColumns:\n",
        "    encode_text_dummy(train_df, col)\n",
        "    encode_text_dummy(test_df, col)\n",
        "\n",
        "# Ensure training and test sets have the same columns after encoding\n",
        "train_df, test_df = train_df.align(test_df, join='inner', axis=1)\n",
        "\n",
        "# Send the expected numeric and categorical columns to normalization. Send target to text_index.\n",
        "NumericColumns = ['dur','spkts','dpkts','sbytes','dbytes','rate','sttl','dttl','sload','dload','sloss','dloss','sinpkt','dinpkt','sjit','djit','swin','stcpb','dtcpb','tcprtt',\n",
        "               'synack','ackdat','smean','dmean','trans_depth','response_body_len','ct_srv_src','ct_state_ttl','ct_dst_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','is_ftp_login','ct_ftp_cmd',\n",
        "               'ct_flw_http_mthd','ct_src_ltm','ct_srv_dst','is_sm_ips_ports']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_df[NumericColumns] = scaler.fit_transform(train_df[NumericColumns])\n",
        "test_df[NumericColumns] = scaler.transform(test_df[NumericColumns])\n",
        "\n",
        "# Now convert the DataFrame to feature matrix (x) and target vector (y)\n",
        "x, y = to_xy(train_df, \"attack_cat\")\n",
        "x_test, y_test = to_xy(test_df, \"attack_cat\")\n",
        "\n",
        "print(train_df['attack_cat'].value_counts())\n",
        "print(test_df['attack_cat'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwRfwOC74Kys"
      },
      "source": [
        "### Drop Unique Rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "VQDJZTv54Kys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "4c2aea8b-da42-4789-aaac-b501b2a0e565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique categories in Train (before conversion): [0 1]\n",
            "Unique categories in Test (before conversion): [0 1]\n",
            "Unique categories in Train: set()\n",
            "Unique categories in Test: set()\n",
            "Train categories: 2\n",
            "Test categories: 2\n",
            "attack_cat\n",
            "1    61681\n",
            "0    19478\n",
            "Name: count, dtype: int64\n",
            "attack_cat\n",
            "1    25554\n",
            "0     9624\n",
            "Name: count, dtype: int64\n",
            "Before fix [0 1]\n",
            "attack_cat\n",
            "1    61681\n",
            "0    19478\n",
            "Name: count, dtype: int64\n",
            "attack_cat\n",
            "1    25554\n",
            "0     9624\n",
            "Name: count, dtype: int64\n",
            "[0 1]\n",
            "Updated Train categories: 2\n",
            "Updated Test categories: 2\n"
          ]
        }
      ],
      "source": [
        "print(\"Unique categories in Train (before conversion):\", train_df['attack_cat'].unique())\n",
        "print(\"Unique categories in Test (before conversion):\", test_df['attack_cat'].unique())\n",
        "\n",
        "train_categories = set(train_df['attack_cat'].unique())\n",
        "test_categories = set(test_df['attack_cat'].unique())\n",
        "\n",
        "unique_to_train = train_categories - test_categories\n",
        "unique_to_test = test_categories - train_categories\n",
        "\n",
        "print(\"Unique categories in Train:\", unique_to_train)\n",
        "print(\"Unique categories in Test:\", unique_to_test)\n",
        "\n",
        "# Drop unique categories in the train dataset\n",
        "train_df = train_df[~train_df['attack_cat'].isin(unique_to_train)]\n",
        "\n",
        "# Drop unique categories in the test dataset\n",
        "test_df = test_df[~test_df['attack_cat'].isin(unique_to_test)]\n",
        "\n",
        "print(\"Train categories:\", train_df['attack_cat'].nunique())\n",
        "print(\"Test categories:\", test_df['attack_cat'].nunique())\n",
        "print(train_df['attack_cat'].value_counts())\n",
        "print(test_df['attack_cat'].value_counts())\n",
        "print(\"Before fix\",train_df['attack_cat'].unique())\n",
        "\n",
        "#Fix to binary problem where 1 means safe and 0 means attack\n",
        "train_df['attack_cat'] = train_df['attack_cat'].apply(lambda x: 1 if x != 0 else 0)\n",
        "test_df['attack_cat'] = test_df['attack_cat'].apply(lambda x: 1 if x != 0 else 0)\n",
        "print(train_df['attack_cat'].value_counts())\n",
        "print(test_df['attack_cat'].value_counts())\n",
        "print(train_df['attack_cat'].unique())\n",
        "\n",
        "print(\"Updated Train categories:\", train_df['attack_cat'].nunique())\n",
        "print(\"Updated Test categories:\", test_df['attack_cat'].nunique())\n",
        "# Assuming you have a DataFrame called df and want to print unique values in a specific column\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU0RIxcm4Kys"
      },
      "source": [
        "### Make First Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "9O7IhOxu4Kyt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b49730e9-03c9-4a5f-cb64-3987930fcbf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLASS\n",
            "CLASS\n",
            "Unique values in y_train_binary: [0. 1.]\n",
            "Counts in y_train_binary: [19478 61681]\n",
            "\n",
            "Unique values in y_test_binary: [0. 1.]\n",
            "Counts in y_test_binary: [ 9624 25554]\n",
            "Train X shape: (81159, 59)\n",
            "Train Y shape: (81159, 1)\n",
            "Test X shape: (35178, 59)\n",
            "Test Y shape: (35178, 1)\n",
            "attack_cat\n",
            "1    61681\n",
            "0    19478\n",
            "Name: count, dtype: int64\n",
            "attack_cat\n",
            "1    25554\n",
            "0     9624\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2537/2537 - 13s - 5ms/step - accuracy: 0.7817 - loss: 24.4319 - val_accuracy: 0.5041 - val_loss: 0.8134\n",
            "Epoch 2/100\n",
            "2537/2537 - 5s - 2ms/step - accuracy: 0.8343 - loss: 0.4553 - val_accuracy: 0.3651 - val_loss: 0.9460\n",
            "Epoch 3/100\n",
            "2537/2537 - 6s - 2ms/step - accuracy: 0.8627 - loss: 0.3051 - val_accuracy: 0.3188 - val_loss: 1.0934\n",
            "Epoch 4/100\n",
            "2537/2537 - 5s - 2ms/step - accuracy: 0.8673 - loss: 0.3064 - val_accuracy: 0.3486 - val_loss: 2.5481\n",
            "Epoch 5/100\n",
            "2537/2537 - 12s - 5ms/step - accuracy: 0.8580 - loss: 0.3177 - val_accuracy: 0.4759 - val_loss: 3.3150\n",
            "Epoch 6/100\n",
            "2537/2537 - 9s - 4ms/step - accuracy: 0.8635 - loss: 0.3093 - val_accuracy: 0.4201 - val_loss: 5.5953\n",
            "Epoch 6: early stopping\n",
            "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
            "Accuracy score: 0.5040934675081017\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.32      0.75      0.45      9624\n",
            "         1.0       0.81      0.41      0.55     25554\n",
            "\n",
            "    accuracy                           0.50     35178\n",
            "   macro avg       0.57      0.58      0.50     35178\n",
            "weighted avg       0.68      0.50      0.52     35178\n",
            "\n",
            "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.32      0.75      0.45      9624\n",
            "      Attack       0.81      0.41      0.55     25554\n",
            "\n",
            "    accuracy                           0.50     35178\n",
            "   macro avg       0.57      0.58      0.50     35178\n",
            "weighted avg       0.68      0.50      0.52     35178\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Figure out how to turn the output into 0 and 1, not 0-7\n",
        "x,y = to_xy(train_df,\"attack_cat\")\n",
        "x_test,y_test = to_xy(test_df,\"attack_cat\")\n",
        "\n",
        "unique_train, counts_train = np.unique(y, return_counts=True)\n",
        "print(\"Unique values in y_train_binary:\", unique_train)\n",
        "print(\"Counts in y_train_binary:\", counts_train)\n",
        "\n",
        "# Print unique values and their counts for y_test_binary\n",
        "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
        "print(\"\\nUnique values in y_test_binary:\", unique_test)\n",
        "print(\"Counts in y_test_binary:\", counts_test)\n",
        "print(\"Train X shape:\", x.shape)\n",
        "print(\"Train Y shape:\", y.shape)\n",
        "print(\"Test X shape:\", x_test.shape)\n",
        "print(\"Test Y shape:\", y_test.shape)\n",
        "print(train_df['attack_cat'].value_counts())\n",
        "print(test_df['attack_cat'].value_counts())\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(100, input_dim=x.shape[1], activation='relu')) # More neurons\n",
        "model.add(Dropout(0.1))  # Dropout to reduce overfitting\n",
        "model.add(Dense(50, activation='relu')) # Additional layer\n",
        "model.add(Dropout(0.1))  # Dropout to reduce overfitting\n",
        "model.add(Dense(25, activation='relu'))\n",
        "model.add(Dropout(0.1))  # Dropout to reduce overfitting\n",
        "model.add(Dense(1, activation='sigmoid')) # Output layer\n",
        "model.save('dnn/model.keras')\n",
        "adam = optimizers.Adam(learning_rate=0.001, beta_1=0.999, beta_2=0.999, epsilon=None, amsgrad=False)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=2, mode='auto')\n",
        "model.save('dnn/model.keras')\n",
        "checkpointer = ModelCheckpoint(filepath=\"dnn/model.keras\", verbose=0, save_best_only=True) # save best model\n",
        "model.fit(x, y, validation_data=(x_test,y_test), callbacks=[monitor, checkpointer], verbose=2, epochs=100)\n",
        "model.load_weights('dnn/model.keras') # load weights from best model\n",
        "#model.fit(x,y,verbose=2,epochs=10)\n",
        "\n",
        "# Make predictions on the test set\n",
        "pred = model.predict(x_test)\n",
        "\n",
        "predicted_classes = (pred > 0.5).astype(int).flatten()  # Ensure it's 1D\n",
        "true_classes = y_test.flatten()  # Ensure it's 1D\n",
        "\n",
        "# Evaluate the accuracy score\n",
        "score = metrics.accuracy_score(true_classes, predicted_classes)\n",
        "print(\"Accuracy score: {}\".format(score))\n",
        "\n",
        "# Print classification report\n",
        "print(metrics.classification_report(true_classes, predicted_classes))\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_probs = model.predict(x_test).flatten()\n",
        "y_pred_classes = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred_classes, target_names=['Normal', 'Attack']))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}